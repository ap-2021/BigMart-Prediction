{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjrNsgOATWVQ"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ydata-profiling"
      ],
      "metadata": {
        "id": "GeH2RnNcY484"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import numpy as np\n",
        "import shap\n",
        "import time\n",
        "import optuna\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "from sklearn.inspection import partial_dependence\n",
        "\n",
        "from contextlib import contextmanager\n",
        "\n",
        "\n",
        "from ydata_profiling import ProfileReport\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "012PMmBPUSr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "693d354c"
      },
      "source": [
        "\n",
        "\n",
        "# Read train and test data\n",
        "df_sales_train = pd.read_csv('/content/train.csv')\n",
        "df_sales_test = pd.read_csv('/content/test.csv')\n",
        "\n",
        "df_sales_train['split'] = 'train'\n",
        "df_sales_test['split'] = 'test'\n",
        "df_sales_test['Item_Outlet_Sales'] = 0\n",
        "\n",
        "\n",
        "df_all=pd.concat([df_sales_train,df_sales_test])\n",
        "display(df_all.tail())\n",
        "# df_sales_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "px1Q1P3RDnJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA\n",
        "\n",
        "profile = ProfileReport(df_sales_train)\n",
        "profile.to_file(\"bigmart_profile.html\")"
      ],
      "metadata": {
        "id": "G6_zXosy_NZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning  and Feature Engineering\n"
      ],
      "metadata": {
        "id": "JDQdz3uLDkZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Item_Fat Content standardization\n",
        "df_all[\"Item_Fat_Content\"] = df_all[\"Item_Fat_Content\"].str.strip().str.lower().str.replace(' ', '')\n",
        "\n",
        "df_all[\"Item_Fat_Content\"] = df_all[\"Item_Fat_Content\"].replace({\n",
        "    \"lf\": \"low fat\",\n",
        "    \"reg\": \"regular\"\n",
        "})"
      ],
      "metadata": {
        "id": "y36UlEOib-f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64383b39"
      },
      "source": [
        "# Imputing Iem weight with mean weight\n",
        "\n",
        "mean_item_weight = df_all.groupby('Item_Identifier')['Item_Weight'].mean()\n",
        "\n",
        "\n",
        "df_all['Item_Weight'] = df_all['Item_Weight'].fillna(df_all['Item_Identifier'].map(mean_item_weight))\n",
        "\n",
        "print(f\"Remaining NaN values in 'Item_Weight' after imputation: {df_all['Item_Weight'].isnull().sum()}\")\n",
        "display(df_all.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales_train['Item_Weight'].isnull().sum()"
      ],
      "metadata": {
        "id": "vjokFIpjrL_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce055268"
      },
      "source": [
        "# Group statistics at outlet level\n",
        "\n",
        "sales_and_outlet_stats = df_all.groupby(['Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type','Outlet_Identifier','Outlet_Establishment_Year'], dropna=False).agg(\n",
        "    total_item_sales=('Item_Outlet_Sales', 'sum'),\n",
        "    distinct_outlets=('Outlet_Identifier', 'nunique'),\n",
        "    distinct_item=('Item_Identifier', 'nunique'),\n",
        ").reset_index()\n",
        "display(sales_and_outlet_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputing outlet size\n",
        "\n",
        "df_all['Outlet_Size']=df_all['Outlet_Size'].fillna('Small')"
      ],
      "metadata": {
        "id": "ECuPK9WzfmIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6ec733c"
      },
      "source": [
        "# Imputing item visibility with mean visibility per item\n",
        "\n",
        "mean_visibility_by_item_type = df_all[df_all['Item_Visibility'] != 0].groupby('Item_Type')['Item_Visibility'].mean()\n",
        "\n",
        "df_all['Item_Visibility_masked'] = df_all['Item_Visibility'].copy()\n",
        "\n",
        "zero_visibility_mask = df_all['Item_Visibility_masked'] == 0\n",
        "\n",
        "df_all.loc[zero_visibility_mask, 'Item_Visibility_masked'] = df_all.loc[zero_visibility_mask, 'Item_Type'].map(mean_visibility_by_item_type)\n",
        "\n",
        "print(f\"Number of 0 values in 'Item_Visibility_masked' after imputation: {(df_all['Item_Visibility_masked'] == 0).sum()}\")\n",
        "display(df_all.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating outlet_age variable\n",
        "\n",
        "df_all['Current_Year'] = datetime.datetime.now().year\n",
        "df_all['Outlet_Age'] = df_all['Current_Year'] - df_all['Outlet_Establishment_Year']"
      ],
      "metadata": {
        "id": "72fqkHSir29Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fba09661"
      },
      "source": [
        "# calculating outlet average sales\n",
        "\n",
        "train_outlet_avg_sales = df_all[df_all['split'] == 'train'].groupby('Outlet_Identifier')['Item_Outlet_Sales'].mean()\n",
        "df_all['Outlet_Average_Sales'] = df_all['Outlet_Identifier'].map(train_outlet_avg_sales)\n",
        "display(df_all.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df_all['Outlet_Average_Sales'] == 0).sum()"
      ],
      "metadata": {
        "id": "2XdlxusBwM-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting item category\n",
        "\n",
        "df_all['item_category'] = df_all['Item_Identifier'].str.slice(0, 2)\n",
        "display(df_all.head())"
      ],
      "metadata": {
        "id": "MbLI-9ZPy0P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6db32803"
      },
      "source": [
        "# calculating item count per outlet\n",
        "\n",
        "outlet_item_count = df_all.groupby('Outlet_Identifier')['Item_Identifier'].nunique().reset_index()\n",
        "outlet_item_count.rename(columns={'Item_Identifier': 'Outlet_Item_Count'}, inplace=True)\n",
        "df_all = pd.merge(df_all, outlet_item_count, on='Outlet_Identifier', how='left')\n",
        "display(df_all.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2bee99b"
      },
      "source": [
        "\n",
        "# calculating average sales per item category per outlet\n",
        "\n",
        "item_cat_avg_sales_per_outlet = df_all[df_all['split'] == 'train'].groupby(['Outlet_Identifier', 'item_category'])['Item_Outlet_Sales'].mean().reset_index()\n",
        "item_cat_avg_sales_per_outlet = item_cat_avg_sales_per_outlet.rename(columns={'Item_Outlet_Sales': 'Item_cat_Avg_Sales_Per_Outlet'})\n",
        "df_all = pd.merge(df_all, item_cat_avg_sales_per_outlet, on=['Outlet_Identifier', 'item_category'], how='left')\n",
        "display(df_all.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "784cd85d"
      },
      "source": [
        "# calculating visibility ratio\n",
        "\n",
        "df_all['Item_Visibility_Mean'] = df_all.groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n",
        "df_all['Visibility_Ratio'] = df_all['Item_Visibility'] / df_all['Item_Visibility_Mean']\n",
        "display(df_all.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "128f968f"
      },
      "source": [
        "# grouping item type\n",
        "\n",
        "df_all['Item_Type_cleaned'] = df_all['Item_Type'].str.strip().str.lower().str.replace(' ', '')\n",
        "\n",
        "def item_type_grouper(item_type):\n",
        "    if item_type in [\n",
        "        'fruitsandvegetables', 'snackfoods', 'household', 'frozenfoods',\n",
        "        'dairy', 'canned', 'bakinggoods', 'healthandhygiene', 'softdrinks', 'meat'\n",
        "    ]:\n",
        "        return item_type\n",
        "    else:\n",
        "        return 'others'\n",
        "\n",
        "df_all['item_type_grouped'] = df_all['Item_Type_cleaned'].apply(item_type_grouper)\n",
        "display(df_all.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all['Outlet_Type_cleaned'] = df_all['Outlet_Type'].str.strip().str.lower().str.replace(' ', '')"
      ],
      "metadata": {
        "id": "-rAJtjaz4-Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all['Outlet_Location_Type_cleaned'] = df_all['Outlet_Location_Type'].str.strip().str.lower().str.replace(' ', '')"
      ],
      "metadata": {
        "id": "vNQy-U3g7zsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# item price bucket created\n",
        "\n",
        "df_all['item_price_bucket'] = pd.qcut(df_all['Item_MRP'], q=4, labels=['Low', 'Med', 'High', 'Premium'])\n",
        "\n",
        "item_price_map = {'Low': 1, 'Med': 2, 'High': 3, 'Premium': 4}\n",
        "df_all['item_price_bucket'] = df_all['item_price_bucket'].map(item_price_map)\n",
        "\n",
        "display(df_all.head())"
      ],
      "metadata": {
        "id": "7AVVx7nTSuCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# interaction variable created: MRP X outlet type\n",
        "\n",
        "outlet_mean = df_all[df_all['split'] == 'train'].groupby('Outlet_Type')['Item_Outlet_Sales'].mean()\n",
        "\n",
        "df_all[\"Outlet_Type_TE\"] = df_all[\"Outlet_Type\"].map(outlet_mean)\n",
        "\n",
        "df_all[\"MRP_x_OutletType\"] = df_all[\"Item_MRP\"] * df_all[\"Outlet_Type_TE\"]\n",
        "display(df_all.head())"
      ],
      "metadata": {
        "id": "lT61hoCFl7kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nonlinear price\n",
        "df_all[\"MRP_squared\"] = df_all[\"Item_MRP\"] ** 2"
      ],
      "metadata": {
        "id": "sHGMmh7f0C58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# other price related features\n",
        "\n",
        "df_all[\"MRP_log\"] = np.log1p(df_all[\"Item_MRP\"])\n",
        "df_all[\"MRP_x_OutletAvg\"] = df_all[\"Item_MRP\"] * df_all[\"Outlet_Average_Sales\"]"
      ],
      "metadata": {
        "id": "MbUSUJ-s7pnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.columns"
      ],
      "metadata": {
        "id": "3Bx-XJjI5A2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical feature encoding"
      ],
      "metadata": {
        "id": "1qPqaAmmEgY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_filtered = df_all.copy()\n",
        "\n",
        "# Define ordinal mapping for Outlet_Size\n",
        "outlet_size_map = {'Small': 0, 'Medium': 1, 'High': 2}\n",
        "df_all_filtered['Outlet_Size'] = df_all_filtered['Outlet_Size'].map(outlet_size_map)\n",
        "\n",
        "fat_map = {'lowfat': 0, 'regular': 1}\n",
        "df_all_filtered['Item_Fat_Content'] = df_all_filtered['Item_Fat_Content'].map(fat_map)\n",
        "\n",
        "# Select the final set of features for the model, now with ordinally encoded Outlet_Size\n",
        "df_all_filtered = df_all_filtered[['Item_Identifier', 'Item_Weight', 'Visibility_Ratio',\n",
        "         'Outlet_Identifier', 'Outlet_Age',\n",
        "        'Outlet_Type_cleaned','item_category',\n",
        "       'split',\n",
        "       'Outlet_Item_Count', 'Item_cat_Avg_Sales_Per_Outlet',\n",
        "       'Item_Outlet_Sales','Item_MRP','Outlet_Average_Sales']]\n",
        "\n",
        "      #  'Outlet_Location_Type_cleaned', 'Item_Fat_Content','item_type_grouped', 'Outlet_Size','','MRP_x_OutletType,,'MRP_squared'\n",
        "      # ,'Item_MRP','MRP_log','Outlet_Average_Sales',\n",
        "\n",
        "display(df_all_filtered.head())"
      ],
      "metadata": {
        "id": "oN3ubwZW7MyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "850d76c8"
      },
      "source": [
        "\n",
        "# removed cat var\n",
        "\n",
        "categorical_cols = [  'Outlet_Type_cleaned','item_category']\n",
        "# , 'Outlet_Location_Type_cleaned','item_type_grouped', 'item_category'\n",
        "# Apply one-hot encoding\n",
        "df_all_filtered = pd.get_dummies(df_all_filtered, columns=categorical_cols, dtype=int)\n",
        "print(f\"Shape of DataFrame after one-hot encoding: {df_all_filtered.shape}\")\n",
        "display(df_all_filtered.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outlet_cols = [col for col in df_all_filtered.columns if \"Outlet_Type_\" in col]\n",
        "\n",
        "# for col in outlet_cols:\n",
        "#     df_all_filtered[f\"MRP_x_{col}\"] = df_all_filtered[\"Item_MRP\"] * df_all_filtered[col]"
      ],
      "metadata": {
        "id": "Yc7DbJkGlUX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split into Train and  Test"
      ],
      "metadata": {
        "id": "m-4QerC4EsQr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3613909"
      },
      "source": [
        "# Remove the 'test' split data\n",
        "df_train_only = df_all_filtered[df_all_filtered['split'] == 'train'].copy()\n",
        "\n",
        "# Drop the 'split' column as it's no longer needed after filtering\n",
        "df_train_only = df_train_only.drop(columns=['split'])\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_train_only.drop(columns=['Item_Outlet_Sales', 'Item_Identifier', 'Outlet_Identifier'])\n",
        "# X = df_train_only.drop(columns=['Item_Outlet_Sales'])\n",
        "y = df_train_only['Item_Outlet_Sales']\n",
        "\n",
        "# print(f\"Shape of features (X) before split: {X.shape}\")\n",
        "print(f\"Shape of target (y) before split: {y.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X=df_train_only.copy()"
      ],
      "metadata": {
        "id": "rZbOxXYtDxEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c823e753"
      },
      "source": [
        "\n",
        "\n",
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define grouping variable\n",
        "# groups = X_train[\"Outlet_Identifier\"]"
      ],
      "metadata": {
        "id": "U-MjBoATAprV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# outlet_mean = X_train.groupby(\"Outlet_Identifier\")[\"Item_Outlet_Sales\"].mean()\n",
        "\n",
        "# X_train[\"Outlet_ID_TE\"] = X_train[\"Outlet_Identifier\"].map(outlet_mean)\n",
        "# X_test[\"Outlet_ID_TE\"] = X_test[\"Outlet_Identifier\"].map(outlet_mean)\n",
        "\n",
        "# X_train = X_train.drop(columns=['Item_Outlet_Sales', 'Item_Identifier', 'Outlet_Identifier'])\n",
        "# X_test = X_test.drop(columns=['Item_Outlet_Sales', 'Item_Identifier', 'Outlet_Identifier'])"
      ],
      "metadata": {
        "id": "CAwBlUUZCYLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns"
      ],
      "metadata": {
        "id": "sOKatH-wEL04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4b51cf1"
      },
      "source": [
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler ONLY on X_train and then transform X_train\n",
        "X_train[['Item_Weight']] = scaler.fit_transform(X_train[['Item_Weight']])\n",
        "# X_train[['Item_Weight','Item_MRP']] = scaler.fit_transform(X_train[['Item_Weight','Item_MRP']])\n",
        "\n",
        "# Transform X_test using the SAME scaler fitted on X_train\n",
        "X_test[['Item_Weight']] = scaler.transform(X_test[['Item_Weight']])\n",
        "\n",
        "print(\"Item_Weight and Item_MRP scaled for both X_train and X_test.\")\n",
        "display(X_train.head())\n",
        "display(X_test.head())\n",
        "\n",
        "# '','MRP_x_OutletType,,'MRP_squared',,'Item_MRP'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# calculate 99th percentile from training target\n",
        "# cap_value = np.percentile(y_train, 99)\n",
        "# y_train_capped = np.clip(y_train, None, cap_value)\n",
        "# y_test_capped = np.clip(y_test, None, cap_value)"
      ],
      "metadata": {
        "id": "SV1UGQ5a1w_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(y_train, bins=50)\n",
        "plt.xlabel(\"Item_Outlet_Sales\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Histogram of True Sales\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9lPPD7rE3FPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# y_train_log = np.log1p(y_train)\n",
        "# y_test_log = np.log1p(y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "v_2Wn8Je3kGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final prediction data prep"
      ],
      "metadata": {
        "id": "wD07jNazx12w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ae2239c"
      },
      "source": [
        "# Filter df_all_filtered to get the test split data\n",
        "df_test_final = df_all_filtered[df_all_filtered['split'] == 'test'].copy()\n",
        "\n",
        "# Prepare features for prediction, similar to how X_train was created\n",
        "# Drop 'Item_Outlet_Sales' (since it's all 0 for test), 'Item_Identifier', 'Outlet_Identifier', and 'split'\n",
        "X_final_test = df_test_final.copy()\n",
        "X_final_test[\"Outlet_ID_TE\"] = X_final_test[\"Outlet_Identifier\"].map(outlet_mean)\n",
        "X_final_test = X_final_test.drop(columns=['Item_Outlet_Sales', 'Item_Identifier', 'Outlet_Identifier', 'split'])\n",
        "\n",
        "# Ensure the columns are in the same order as X_train\n",
        "X_final_test = X_final_test[X_train.columns] # Use X_train.columns to match order and selected features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply the SAME scaler fitted on X_train to the 'Item_Weight' and 'Item_MRP' of the test data\n",
        "X_final_test[['Item_Weight']] = scaler.transform(X_final_test[['Item_Weight']])\n",
        "\n",
        "print(f\"Shape of final test features for prediction: {X_final_test.shape}\")\n",
        "display(X_final_test.head())\n",
        "\n",
        "\n",
        "# '','MRP_x_OutletType,'MRP_squared',,'Item_MRP'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_final_test.columns"
      ],
      "metadata": {
        "id": "o9qFrrfellcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RF Model"
      ],
      "metadata": {
        "id": "hLNyFTgPkicy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold = y_train.quantile(0.75)\n",
        "\n",
        "# weights = np.where(y_train > threshold, 2, 1)\n",
        "\n",
        "# weights = 1 + (y_train / y_train.mean())\n"
      ],
      "metadata": {
        "id": "72PKKmIcyxou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# residuals = np.abs(y_train - rf_model.predict(X_train))\n",
        "\n",
        "# weights = 1 + (residuals / residuals.mean())\n",
        "\n",
        "# weights = weights / np.mean(weights)"
      ],
      "metadata": {
        "id": "F-TWzoeuzYZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cae9e3a"
      },
      "source": [
        "\n",
        "\n",
        "# Initialize the Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=177,\n",
        "    max_depth=6,\n",
        "    min_samples_leaf=7,\n",
        "    max_features= 0.7785524356036123,\n",
        "    random_state=71\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "# rf_model.fit(X_train, y_train, sample_weight=weights)\n",
        "# rf_model.fit(X_train, y_train_capped)\n",
        "\n",
        "print(\"Random Forest model training complete on scaled data.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # n_estimators=100,             # Number of trees in the forest\n",
        "    # random_state=42,              # Random seed for reproducibility\n",
        "    # max_depth=10,                 # Maximum depth of the tree\n",
        "    # min_samples_leaf=5            # Minimum number of samples required to be at a leaf node"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import lightgbm as lgb\n",
        "\n",
        "# model = lgb.LGBMRegressor(\n",
        "#     objective=\"quantile\",\n",
        "#     alpha=0.5,   # 0.5 = median\n",
        "#     n_estimators=2000,\n",
        "#     learning_rate=0.01,\n",
        "#     random_state=56\n",
        "# )\n",
        "\n",
        "# model = lgb.LGBMRegressor(\n",
        "#     objective=\"tweedie\",\n",
        "#     tweedie_variance_power=1.2\n",
        "# )\n",
        "\n",
        "# model = lgb.LGBMRegressor(\n",
        "#     objective=\"regression\",   # default MSE\n",
        "#     n_estimators=2000,\n",
        "#     learning_rate=0.01\n",
        "# )\n",
        "\n",
        "# # model.fit(X_train, y_train)\n",
        "# model.fit(X_train, y_train_log)\n",
        "\n",
        "\n",
        "\n",
        "# lgb_model = lgb.LGBMRegressor(\n",
        "#     objective=\"regression\",\n",
        "#     n_estimators=3000,\n",
        "#     learning_rate=0.01,\n",
        "#     num_leaves=40,\n",
        "#     subsample=0.8,\n",
        "#     colsample_bytree=0.8,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# lgb_model.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "-q2tE1413HYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rf_model=model"
      ],
      "metadata": {
        "id": "L7vFFvQG0tP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a703dec4"
      },
      "source": [
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "# y_pred_rf = model.predict(X_test)\n",
        "\n",
        "# y_pred_log = model.predict(X_test)\n",
        "# y_pred_rf = np.expm1(y_pred_log)\n",
        "\n",
        "# Evaluate the model\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# rmse_rf = np.sqrt(mean_squared_error(y_test_capped, y_pred_rf))\n",
        "# r2_rf = r2_score(y_test_capped, y_pred_rf)\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE) for Random Forest: {rmse_rf:.2f}\")\n",
        "print(f\"R-squared (R2 Score) for Random Forest: {r2_rf:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # for  ensemble model\n",
        "\n",
        "# pred_lgb = lgb_model.predict(X_test)\n",
        "# pred_rf = rf_model.predict(X_test)\n",
        "# final_pred = 0.2 * pred_lgb + 0.8 * pred_rf\n",
        "\n",
        "# rmse = np.sqrt(mean_squared_error(y_test, final_pred))\n",
        "# print(\"Ensemble RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "d6ol_UCL-8kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=rf_model\n",
        "y_pred=y_pred_rf\n"
      ],
      "metadata": {
        "id": "9tm5k8zDy2K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b427c714"
      },
      "source": [
        "\n",
        "\n",
        "# Get feature importances from Random Forest model\n",
        "importance_rf = rf_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_names = X_train.columns\n",
        "df_feature_importance_rf = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importance_rf\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display top N features\n",
        "display(df_feature_importance_rf.head(10))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=df_feature_importance_rf.head(20))\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1725d40"
      },
      "source": [
        "# Make predictions on the prepared final test set using the Random Forest model\n",
        "test_predictions_rf = rf_model.predict(X_final_test)\n",
        "\n",
        "print(\"Predictions for the test dataset using Random Forest model:\")\n",
        "display(pd.Series(test_predictions_rf).head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80e2ff9"
      },
      "source": [
        "# Create a DataFrame for the final predictions with identifiers using Random Forest predictions\n",
        "submission_df_rf = pd.DataFrame({\n",
        "    'Item_Identifier': df_test_final['Item_Identifier'],\n",
        "    'Outlet_Identifier': df_test_final['Outlet_Identifier'],\n",
        "    'Item_Outlet_Sales': test_predictions_rf\n",
        "})\n",
        "\n",
        "display(submission_df_rf.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e0c0386"
      },
      "source": [
        "negative_predictions_count_rf = (test_predictions_rf < 0).sum()\n",
        "print(f\"Number of negative predicted Item_Outlet_Sales values by Random Forest: {negative_predictions_count_rf}\")\n",
        "\n",
        "if negative_predictions_count_rf > 0:\n",
        "    print(\"There are negative predicted values. Consider post-processing or a different model objective if sales cannot be negative.\")\n",
        "else:\n",
        "    print(\"All predicted values by Random Forest are non-negative.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f569c8a1"
      },
      "source": [
        "submission_df_rf.to_csv('submission_random_forest_3.csv', index=False)\n",
        "print(\"submission_random_forest.csv has been saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HyperParameter Tuning"
      ],
      "metadata": {
        "id": "djnNdxLMHZjQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3906852a"
      },
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
        "    max_depth = trial.suggest_int('max_depth', 5, 20)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
        "    max_features = trial.suggest_float('max_features', 0.6, 1.0) # Changed to suggest_float\n",
        "\n",
        "    # Initialize Random Forest Regressor with suggested hyperparameters\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        min_samples_split=min_samples_split,\n",
        "        max_features=max_features,\n",
        "        random_state=42,\n",
        "        n_jobs=-1 # Use all available cores\n",
        "    )\n",
        "\n",
        "    # Use KFold for cross-validation to get a robust evaluation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros(len(X_train))\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
        "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index] # Use raw y_train\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "        fold_preds = model.predict(X_val_fold)\n",
        "        oof_preds[val_index] = np.clip(fold_preds, a_min=0, a_max=None) # Clip predictions to be non-negative\n",
        "\n",
        "    # Calculate RMSE directly on the original scale\n",
        "    rmse = np.sqrt(mean_squared_error(y_train, oof_preds))\n",
        "\n",
        "    return rmse\n",
        "\n",
        "print(\"Objective function for Optuna defined, using raw target values and clipping predictions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8743cc0f"
      },
      "source": [
        "study = optuna.create_study(direction='minimize') # Minimize RMSE\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"Optimization finished.\")\n",
        "print(\"Best trial:\")\n",
        "print(f\"  Value: {study.best_value:.2f} (RMSE)\")\n",
        "print(\"  Params: \")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Error Analysis"
      ],
      "metadata": {
        "id": "b7rNslDXHg6e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e09ee294"
      },
      "source": [
        "\n",
        "# generating val data for  error analysis\n",
        "\n",
        "df_results = X_test.copy()\n",
        "\n",
        "# Get Item_Identifier and Outlet_Identifier for the test set\n",
        "df_results['Item_Identifier'] = df_train_only.loc[X_test.index, 'Item_Identifier']\n",
        "df_results['Outlet_Identifier'] = df_train_only.loc[X_test.index, 'Outlet_Identifier']\n",
        "\n",
        "df_results['Actual_Item_Outlet_Sales'] = y_test\n",
        "df_results['Predicted_Item_Outlet_Sales_RF'] = y_pred_rf\n",
        "\n",
        "print(\"Combined DataFrame 'df_results' created with actual and predicted sales, including identifiers.\")\n",
        "display(df_results.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_results.to_csv('df_results_test.csv', index=False)"
      ],
      "metadata": {
        "id": "PHNGaPRcwS_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "@contextmanager\n",
        "def time_block(step_name, timings, verbose=True):\n",
        "    if verbose:\n",
        "        print(f\"\\n[START] {step_name}\")\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    yield\n",
        "    end = time.perf_counter()\n",
        "\n",
        "    elapsed = round(end - start, 3)\n",
        "    timings[step_name] = elapsed\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DONE ] {step_name} â€” {elapsed} sec\")\n"
      ],
      "metadata": {
        "id": "9xpZCKylynmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measurements"
      ],
      "metadata": {
        "id": "MTUXPgLWzEqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 1. BASIC ERROR METRICS\n",
        "# =========================================================\n",
        "\n",
        "# def compute_errors(y_true, y_pred):\n",
        "#     df = pd.DataFrame({\n",
        "#         \"y_true\": y_true,\n",
        "#         \"y_pred\": y_pred\n",
        "#     })\n",
        "#     df[\"error\"] = df[\"y_true\"] - df[\"y_pred\"]\n",
        "#     df[\"abs_error\"] = df[\"error\"].abs()\n",
        "#     df[\"sq_error\"] = df[\"error\"] ** 2\n",
        "#     df[\"pct_error\"] = df[\"abs_error\"] / (df[\"y_true\"].abs() + 1e-6)\n",
        "#     return df\n",
        "\n",
        "def compute_errors(df, y_true_col, y_pred_col):\n",
        "    df_err = df.copy()\n",
        "\n",
        "    df_err[\"error\"] = df_err[y_true_col] - df_err[y_pred_col]\n",
        "    df_err[\"abs_error\"] = df_err[\"error\"].abs()\n",
        "    df_err[\"sq_error\"] = df_err[\"error\"] ** 2\n",
        "    df_err[\"pct_error\"] = (\n",
        "        df_err[\"abs_error\"] / (df_err[y_true_col].abs() + 1e-6)\n",
        "    )\n",
        "\n",
        "    return df_err\n",
        "\n",
        "\n",
        "\n",
        "def global_error_summary(df_err):\n",
        "    return {\n",
        "        \"mae\": float(df_err[\"abs_error\"].mean()),\n",
        "        \"rmse\": float(np.sqrt(df_err[\"sq_error\"].mean())),\n",
        "        \"median_abs_error\": float(df_err[\"abs_error\"].median()),\n",
        "        \"mean_error\": float(df_err[\"error\"].mean()),\n",
        "        \"error_std\": float(df_err[\"error\"].std())\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2. ERROR vs TARGET ANALYSIS\n",
        "# =========================================================\n",
        "\n",
        "def error_vs_target_analysis(df_err):\n",
        "    corr_abs = df_err[\"abs_error\"].corr(df_err[\"y_true\"])\n",
        "    corr_pct = df_err[\"pct_error\"].corr(df_err[\"y_true\"])\n",
        "\n",
        "    if corr_abs > 0.3:\n",
        "        pattern = \"errors_grow_with_target\"\n",
        "    elif corr_pct < -0.3:\n",
        "        pattern = \"errors_high_for_small_values\"\n",
        "    else:\n",
        "        pattern = \"no_clear_target_pattern\"\n",
        "\n",
        "    return {\n",
        "        \"corr_abs_error_vs_target\": float(corr_abs),\n",
        "        \"corr_pct_error_vs_target\": float(corr_pct),\n",
        "        \"pattern_hint\": pattern\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3. ERROR vs FEATURE (CLASSICAL)\n",
        "# =========================================================\n",
        "\n",
        "def feature_error_correlations(X, df_err):\n",
        "    rows = []\n",
        "    for col in X.columns:\n",
        "        if np.issubdtype(X[col].dtype, np.number):\n",
        "            corr = X[col].corr(df_err[\"abs_error\"])\n",
        "            rows.append({\n",
        "                \"feature\": col,\n",
        "                \"abs_error_corr\": corr\n",
        "            })\n",
        "    return (\n",
        "        pd.DataFrame(rows)\n",
        "        .sort_values(\"abs_error_corr\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def error_binned_by_feature(X, df_err, feature, bins=5):\n",
        "    df = pd.concat([X[[feature]], df_err], axis=1).dropna()\n",
        "    df[\"bin\"] = pd.qcut(df[feature], bins, duplicates=\"drop\")\n",
        "    return df.groupby(\"bin\")[\"abs_error\"].mean().reset_index()\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4. WORST CASES\n",
        "# =========================================================\n",
        "\n",
        "def get_worst_cases(X, df_err, top_n=20):\n",
        "    df = pd.concat([X, df_err], axis=1)\n",
        "    return df.sort_values(\"abs_error\", ascending=False).head(top_n)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 5. PDP (GLOBAL BEHAVIOR)\n",
        "# =========================================================\n",
        "\n",
        "def compute_pdp(model, X, feature, grid_points=20):\n",
        "    \"\"\"\n",
        "    Version-safe PDP computation for sklearn >=1.0\n",
        "    \"\"\"\n",
        "    pdp = partial_dependence(\n",
        "        model,\n",
        "        X,\n",
        "        [feature],\n",
        "        grid_resolution=grid_points,\n",
        "        kind=\"average\"\n",
        "    )\n",
        "\n",
        "    # sklearn >= 1.2 returns Bunch\n",
        "    if hasattr(pdp, \"grid_values\"):\n",
        "        grid = pdp.grid_values[0]\n",
        "        values = pdp.average[0]\n",
        "    else:\n",
        "        # fallback for older versions\n",
        "        grid = pdp[\"values\"][0]\n",
        "        values = pdp[\"average\"][0]\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"feature\": feature,\n",
        "        \"grid_value\": grid,\n",
        "        \"pdp_value\": values\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "def summarize_pdp(pdp_df):\n",
        "    vals = pdp_df[\"pdp_value\"].values\n",
        "\n",
        "    slope = np.mean(np.diff(vals))\n",
        "    curvature = np.mean(np.diff(np.diff(vals)))\n",
        "    pd_range = vals.max() - vals.min()\n",
        "\n",
        "    if np.all(np.diff(vals) > 0):\n",
        "        shape = \"monotonic_increasing\"\n",
        "    elif np.all(np.diff(vals) < 0):\n",
        "        shape = \"monotonic_decreasing\"\n",
        "    else:\n",
        "        shape = \"non_monotonic\"\n",
        "\n",
        "    return {\n",
        "        \"pd_range\": float(pd_range),\n",
        "        \"mean_slope\": float(slope),\n",
        "        \"curvature\": float(curvature),\n",
        "        \"shape\": shape\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 6. SHAP VALUES\n",
        "# =========================================================\n",
        "\n",
        "def compute_shap_values(model, X, max_samples=5000):\n",
        "    if X.shape[0] > max_samples:\n",
        "        X_sample = X.sample(max_samples, random_state=42)\n",
        "    else:\n",
        "        X_sample = X.copy()\n",
        "\n",
        "    try:\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(X_sample)\n",
        "    except Exception:\n",
        "        explainer = shap.KernelExplainer(\n",
        "            model.predict, shap.sample(X_sample, 100)\n",
        "        )\n",
        "        shap_values = explainer.shap_values(X_sample, nsamples=100)\n",
        "\n",
        "    shap_df = pd.DataFrame(\n",
        "        shap_values,\n",
        "        columns=X_sample.columns,\n",
        "        index=X_sample.index\n",
        "    )\n",
        "\n",
        "    return shap_df, X_sample\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 7. SHAP vs ERROR (CORE)\n",
        "# =========================================================\n",
        "\n",
        "def shap_error_correlations(shap_df, df_err):\n",
        "    rows = []\n",
        "    for col in shap_df.columns:\n",
        "        corr = shap_df[col].abs().corr(\n",
        "            df_err.loc[shap_df.index, \"abs_error\"]\n",
        "        )\n",
        "        rows.append({\n",
        "            \"feature\": col,\n",
        "            \"shap_abs_error_corr\": corr\n",
        "        })\n",
        "    return (\n",
        "        pd.DataFrame(rows)\n",
        "        .sort_values(\"shap_abs_error_corr\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def shap_strength_vs_error(shap_df, df_err):\n",
        "    shap_strength = shap_df.abs().sum(axis=1)\n",
        "    return pd.DataFrame({\n",
        "        \"shap_strength\": shap_strength,\n",
        "        \"abs_error\": df_err.loc[shap_strength.index, \"abs_error\"]\n",
        "    })\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 8. SHAP DEPENDENCE (ERROR-AWARE, NUMERICAL)\n",
        "# =========================================================\n",
        "\n",
        "def shap_dependence_error_summary(shap_df, X, df_err, feature, bins=6):\n",
        "    \"\"\"\n",
        "    Numerical summary of SHAP dependence colored by error.\n",
        "    \"\"\"\n",
        "    df = pd.concat([\n",
        "        X[[feature]].rename(columns={feature: \"feature_value\"}),\n",
        "        shap_df[[feature]].rename(columns={feature: \"shap_value\"}),\n",
        "        df_err[[\"abs_error\"]]\n",
        "    ], axis=1).dropna()\n",
        "\n",
        "    # IMPORTANT: pass a Series, not DataFrame\n",
        "    df[\"feature_bin\"] = pd.qcut(\n",
        "        df[\"feature_value\"],\n",
        "        bins,\n",
        "        duplicates=\"drop\"\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        df.groupby(\"feature_bin\")\n",
        "        .agg(\n",
        "            mean_feature_value=(\"feature_value\", \"mean\"),\n",
        "            mean_shap_value=(\"shap_value\", \"mean\"),\n",
        "            mean_abs_error=(\"abs_error\", \"mean\")\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 9. SHAP SUMMARY vs ERROR BUCKETS\n",
        "# =========================================================\n",
        "\n",
        "def shap_summary_by_error_bucket(shap_df, df_err, n_buckets=4):\n",
        "    df = shap_df.copy()\n",
        "    df[\"abs_error\"] = df_err.loc[shap_df.index, \"abs_error\"]\n",
        "\n",
        "    df[\"error_bucket\"] = pd.qcut(\n",
        "        df[\"abs_error\"],\n",
        "        n_buckets,\n",
        "        labels=[f\"bucket_{i+1}\" for i in range(n_buckets)]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        df.groupby(\"error_bucket\")\n",
        "        .mean()\n",
        "        .abs()\n",
        "        .T\n",
        "    )\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 10. SHAP INTERACTION vs ERROR\n",
        "# =========================================================\n",
        "\n",
        "def shap_interaction_vs_error(model, X, df_err, top_features, max_samples=2000):\n",
        "    if X.shape[0] > max_samples:\n",
        "        X_sample = X.sample(max_samples, random_state=42)\n",
        "    else:\n",
        "        X_sample = X.copy()\n",
        "\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    interaction_vals = explainer.shap_interaction_values(X_sample)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, f1 in enumerate(top_features):\n",
        "        for j, f2 in enumerate(top_features):\n",
        "            if j <= i:\n",
        "                continue\n",
        "\n",
        "            interaction_strength = np.abs(interaction_vals[:, i, j])\n",
        "            corr = np.corrcoef(\n",
        "                interaction_strength,\n",
        "                df_err.loc[X_sample.index, \"abs_error\"]\n",
        "            )[0, 1]\n",
        "\n",
        "            results.append({\n",
        "                \"feature_1\": f1,\n",
        "                \"feature_2\": f2,\n",
        "                \"interaction_error_corr\": corr\n",
        "            })\n",
        "\n",
        "    return (\n",
        "        pd.DataFrame(results)\n",
        "        .sort_values(\"interaction_error_corr\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "###DATA FRAME OUTPUT#####\n",
        "\n",
        "def build_dataframe_outputs(results):\n",
        "    \"\"\"\n",
        "    Convert all important outputs into pandas DataFrames\n",
        "    for inspection, export, and future LLM usage.\n",
        "    \"\"\"\n",
        "    dfs = {}\n",
        "\n",
        "    # ---------------- Core error table ----------------\n",
        "    dfs[\"errors\"] = results[\"df_errors\"].copy()\n",
        "\n",
        "    # ---------------- Feature error correlations ----------------\n",
        "    dfs[\"feature_error_correlations\"] = (\n",
        "        results[\"feature_error_correlations\"].copy()\n",
        "    )\n",
        "\n",
        "    # ---------------- Worst cases ----------------\n",
        "    dfs[\"worst_cases\"] = results[\"worst_cases\"].copy()\n",
        "\n",
        "    # ---------------- PDP raw (stacked) ----------------\n",
        "    pdp_rows = []\n",
        "    for feat, df in results[\"pdp_raw\"].items():\n",
        "        tmp = df.copy()\n",
        "        tmp[\"feature\"] = feat\n",
        "        pdp_rows.append(tmp)\n",
        "\n",
        "    if pdp_rows:\n",
        "        dfs[\"pdp_raw\"] = pd.concat(pdp_rows, ignore_index=True)\n",
        "    else:\n",
        "        dfs[\"pdp_raw\"] = pd.DataFrame()\n",
        "\n",
        "    # ---------------- PDP summary ----------------\n",
        "    dfs[\"pdp_summary\"] = (\n",
        "        pd.DataFrame(results[\"pdp_summary\"])\n",
        "        .T\n",
        "        .reset_index()\n",
        "        .rename(columns={\"index\": \"feature\"})\n",
        "    )\n",
        "\n",
        "    # ---------------- SHAP values ----------------\n",
        "    dfs[\"shap_values\"] = results[\"shap_values\"].copy()\n",
        "\n",
        "    # ---------------- SHAP vs error correlation ----------------\n",
        "    dfs[\"shap_error_correlations\"] = (\n",
        "        results[\"shap_error_correlations\"].copy()\n",
        "    )\n",
        "\n",
        "    # ---------------- SHAP strength vs error ----------------\n",
        "    dfs[\"shap_strength_vs_error\"] = (\n",
        "        results[\"shap_strength_vs_error\"].copy()\n",
        "    )\n",
        "\n",
        "    # ---------------- SHAP dependence summaries ----------------\n",
        "    dep_rows = []\n",
        "    for feat, df in results[\"shap_dependence_summaries\"].items():\n",
        "        tmp = df.copy()\n",
        "        tmp[\"feature\"] = feat\n",
        "        dep_rows.append(tmp)\n",
        "\n",
        "    if dep_rows:\n",
        "        dfs[\"shap_dependence_summary\"] = pd.concat(\n",
        "            dep_rows, ignore_index=True\n",
        "        )\n",
        "    else:\n",
        "        dfs[\"shap_dependence_summary\"] = pd.DataFrame()\n",
        "\n",
        "    # ---------------- SHAP error buckets ----------------\n",
        "    dfs[\"shap_error_buckets\"] = (\n",
        "        results[\"shap_error_buckets\"]\n",
        "        .reset_index()\n",
        "        .rename(columns={\"index\": \"feature\"})\n",
        "    )\n",
        "\n",
        "    # ---------------- SHAP interaction vs error ----------------\n",
        "    dfs[\"shap_interaction_error\"] = (\n",
        "        results[\"shap_interaction_error_corr\"].copy()\n",
        "    )\n",
        "\n",
        "    # ---------------- Timings ----------------\n",
        "    dfs[\"timings\"] = (\n",
        "        pd.DataFrame.from_dict(\n",
        "            results[\"timings\"], orient=\"index\", columns=[\"seconds\"]\n",
        "        )\n",
        "        .reset_index()\n",
        "        .rename(columns={\"index\": \"step\"})\n",
        "    )\n",
        "\n",
        "    return dfs\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 11. MASTER PIPELINE (STEP-1 ONLY)\n",
        "# =========================================================\n",
        "\n",
        "\n",
        "\n",
        "def run_error_analysis_regression(\n",
        "    model,\n",
        "    X,\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    pdp_top_k=2,\n",
        "    shap_top_k=2\n",
        "):\n",
        "    timings = {}\n",
        "\n",
        "    # with time_block(\"01_compute_errors\", timings):\n",
        "    #     df_err = compute_errors(y_true, y_pred)\n",
        "\n",
        "\n",
        "    with time_block(\"01_compute_errors\", timings):\n",
        "      df_base = X.copy()\n",
        "      df_base[\"y_true\"] = y_true\n",
        "      df_base[\"y_pred\"] = y_pred\n",
        "\n",
        "      df_err = compute_errors(\n",
        "          df_base,\n",
        "          y_true_col=\"y_true\",\n",
        "          y_pred_col=\"y_pred\"\n",
        "      )\n",
        "\n",
        "    with time_block(\"02_global_error_summary\", timings):\n",
        "        global_summary = global_error_summary(df_err)\n",
        "\n",
        "    with time_block(\"03_error_vs_target_analysis\", timings):\n",
        "        target_analysis = error_vs_target_analysis(df_err)\n",
        "\n",
        "    with time_block(\"04_feature_error_correlations\", timings):\n",
        "        feature_corr_df = feature_error_correlations(X, df_err)\n",
        "\n",
        "    with time_block(\"05_worst_cases\", timings):\n",
        "        worst_cases = get_worst_cases(X, df_err)\n",
        "\n",
        "    # ---------------- PDP ----------------\n",
        "    pdp_raw = {}\n",
        "    pdp_summary = {}\n",
        "\n",
        "    top_pdp_features = feature_corr_df.head(pdp_top_k)[\"feature\"]\n",
        "\n",
        "    with time_block(\"06_PDP_computation\", timings):\n",
        "        for feat in top_pdp_features:\n",
        "            pdp_df = compute_pdp(model, X, feat)\n",
        "            pdp_raw[feat] = pdp_df\n",
        "            pdp_summary[feat] = summarize_pdp(pdp_df)\n",
        "\n",
        "    # ---------------- SHAP ----------------\n",
        "    with time_block(\"07_SHAP_values\", timings):\n",
        "        shap_df, X_shap = compute_shap_values(model, X)\n",
        "\n",
        "    with time_block(\"08_SHAP_error_correlations\", timings):\n",
        "        shap_error_corr = shap_error_correlations(shap_df, df_err)\n",
        "\n",
        "    with time_block(\"09_SHAP_strength_vs_error\", timings):\n",
        "        shap_strength_df = shap_strength_vs_error(shap_df, df_err)\n",
        "\n",
        "    top_shap_features = shap_error_corr.head(shap_top_k)[\"feature\"].tolist()\n",
        "\n",
        "    with time_block(\"10_SHAP_dependence_error_summary\", timings):\n",
        "        shap_dependence_summaries = {\n",
        "            feat: shap_dependence_error_summary(\n",
        "                shap_df, X_shap, df_err, feat\n",
        "            )\n",
        "            for feat in top_shap_features\n",
        "        }\n",
        "\n",
        "    with time_block(\"11_SHAP_error_buckets\", timings):\n",
        "        shap_error_buckets = shap_summary_by_error_bucket(\n",
        "            shap_df, df_err\n",
        "        )\n",
        "\n",
        "    with time_block(\"12_SHAP_interaction_vs_error\", timings):\n",
        "        shap_interactions = shap_interaction_vs_error(\n",
        "            model, X, df_err, top_shap_features\n",
        "        )\n",
        "\n",
        "    with time_block(\"13_LLM_ready_summary_build\", timings):\n",
        "        llm_ready_summary = {\n",
        "            \"global_error_summary\": global_summary,\n",
        "            \"target_error_analysis\": target_analysis,\n",
        "            \"top_error_features\": (\n",
        "                feature_corr_df\n",
        "                .head(10)\n",
        "                .set_index(\"feature\")[\"abs_error_corr\"]\n",
        "                .to_dict()\n",
        "            ),\n",
        "            \"pdp_summary\": pdp_summary,\n",
        "            \"shap_error_associations\": (\n",
        "                shap_error_corr\n",
        "                .head(10)\n",
        "                .set_index(\"feature\")[\"shap_abs_error_corr\"]\n",
        "                .to_dict()\n",
        "            ),\n",
        "            \"shap_interaction_error_signals\": (\n",
        "                shap_interactions.head(10).to_dict(orient=\"records\")\n",
        "            ),\n",
        "            \"shap_dependence_error_ranges\": {\n",
        "                k: v.to_dict(orient=\"records\")\n",
        "                for k, v in shap_dependence_summaries.items()\n",
        "            },\n",
        "            \"shap_importance_by_error_bucket\": (\n",
        "                shap_error_buckets.iloc[:10].to_dict()\n",
        "            )\n",
        "        }\n",
        "\n",
        "    timings[\"TOTAL_RUNTIME\"] = round(sum(timings.values()), 3)\n",
        "\n",
        "    # return {\n",
        "    #     \"df_errors\": df_err,\n",
        "    #     \"feature_error_correlations\": feature_corr_df,\n",
        "    #     \"worst_cases\": worst_cases,\n",
        "    #     \"pdp_raw\": pdp_raw,\n",
        "    #     \"pdp_summary\": pdp_summary,\n",
        "    #     \"shap_values\": shap_df,\n",
        "    #     \"shap_error_correlations\": shap_error_corr,\n",
        "    #     \"shap_strength_vs_error\": shap_strength_df,\n",
        "    #     \"shap_dependence_summaries\": shap_dependence_summaries,\n",
        "    #     \"shap_error_buckets\": shap_error_buckets,\n",
        "    #     \"shap_interaction_error_corr\": shap_interactions,\n",
        "    #     \"llm_ready_summary\": llm_ready_summary,\n",
        "    #     \"timings\": timings\n",
        "    # }\n",
        "\n",
        "    results = {\n",
        "    \"df_errors\": df_err,\n",
        "    \"global_summary\": global_summary,\n",
        "    \"target_analysis\": target_analysis,\n",
        "    \"feature_error_correlations\": feature_corr_df,\n",
        "    \"worst_cases\": worst_cases,\n",
        "    \"pdp_raw\": pdp_raw,\n",
        "    \"pdp_summary\": pdp_summary,\n",
        "    \"shap_values\": shap_df,\n",
        "    \"shap_error_correlations\": shap_error_corr,\n",
        "    \"shap_strength_vs_error\": shap_strength_df,\n",
        "    \"shap_dependence_summaries\": shap_dependence_summaries,\n",
        "    \"shap_error_buckets\": shap_error_buckets,\n",
        "    \"shap_interaction_error_corr\": shap_interactions,\n",
        "    \"llm_ready_summary\": llm_ready_summary,\n",
        "    \"timings\": timings\n",
        "       }\n",
        "\n",
        "    results[\"outputs_as_dataframes\"] = build_dataframe_outputs(results)\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "0qYZh0HZzBGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_error_analysis_regression(\n",
        "    model=model,\n",
        "    X=X_test,\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred,\n",
        "    pdp_top_k=2,\n",
        "    shap_top_k=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "3dk__xTAzKkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = results[\"outputs_as_dataframes\"]\n",
        "dfs.keys()"
      ],
      "metadata": {
        "id": "RM935NpczT0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs[\"errors\"].head()"
      ],
      "metadata": {
        "id": "jPemIkLDzqDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_err=dfs[\"errors\"].copy()"
      ],
      "metadata": {
        "id": "wr4oMHiw09oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.TARGET SPACE DIAGNOSIS"
      ],
      "metadata": {
        "id": "3UJ9Q06hzy3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# identify outliers/lossdominance/instability\n",
        "\n",
        "def diagnose_error_extremes_df(\n",
        "    df_err,\n",
        "    z_thresh=2.5,\n",
        "    top_frac=0.01,\n",
        "    dominance_thresh=0.3\n",
        "):\n",
        "    \"\"\"\n",
        "    Row-level diagnostics added directly to df_err.\n",
        "\n",
        "    df_err must already contain:\n",
        "    - features\n",
        "    - y_true, y_pred\n",
        "    - error, abs_error, sq_error, pct_error\n",
        "    \"\"\"\n",
        "\n",
        "    df = df_err.copy()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1. OUTLIERS (rarity via absolute error)\n",
        "    # --------------------------------------------------\n",
        "    mu = df[\"abs_error\"].mean()\n",
        "    sigma = df[\"abs_error\"].std() + 1e-9\n",
        "\n",
        "    df[\"abs_error_z\"] = (df[\"abs_error\"] - mu) / sigma\n",
        "    df[\"is_abs_error_outlier\"] = df[\"abs_error_z\"] > z_thresh\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2. LOSS DOMINANCE (severity via squared error)\n",
        "    # --------------------------------------------------\n",
        "    loss_cutoff = df[\"sq_error\"].quantile(1 - top_frac)\n",
        "    df[\"is_loss_dominant\"] = df[\"sq_error\"] >= loss_cutoff\n",
        "\n",
        "    total_loss = df[\"sq_error\"].sum()\n",
        "    dominant_loss = df.loc[df[\"is_loss_dominant\"], \"sq_error\"].sum()\n",
        "\n",
        "    loss_fraction = dominant_loss / (total_loss + 1e-9)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 3. OPTIMIZATION INSTABILITY (global signal)\n",
        "    # --------------------------------------------------\n",
        "    df[\"optimization_instability_flag\"] = (\n",
        "        loss_fraction > dominance_thresh\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 4. Diagnostic tags (human-readable)\n",
        "    # --------------------------------------------------\n",
        "    df[\"diagnostic_tag\"] = \"normal\"\n",
        "\n",
        "    df.loc[df[\"is_abs_error_outlier\"], \"diagnostic_tag\"] = \"rare_extreme_error\"\n",
        "    df.loc[df[\"is_loss_dominant\"], \"diagnostic_tag\"] = \"loss_dominant_row\"\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 5. Summary dataframe\n",
        "    # --------------------------------------------------\n",
        "    diagnostic_summary_df = pd.DataFrame([\n",
        "        {\n",
        "            \"diagnostic\": \"abs_error_outliers\",\n",
        "            \"threshold\": z_thresh,\n",
        "            \"fraction_of_rows\": df[\"is_abs_error_outlier\"].mean(),\n",
        "            \"max_abs_error\": df[\"abs_error\"].max()\n",
        "        },\n",
        "        {\n",
        "            \"diagnostic\": \"loss_dominance\",\n",
        "            \"threshold\": top_frac,\n",
        "            \"fraction_of_rows\": df[\"is_loss_dominant\"].mean(),\n",
        "            \"loss_fraction\": loss_fraction\n",
        "        },\n",
        "        {\n",
        "            \"diagnostic\": \"optimization_instability\",\n",
        "            \"threshold\": dominance_thresh,\n",
        "            \"flag\": loss_fraction > dominance_thresh\n",
        "        }\n",
        "    ])\n",
        "\n",
        "    return df, diagnostic_summary_df\n"
      ],
      "metadata": {
        "id": "zHhSLI0Fzzak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_err already contains features + predictions + errors\n",
        "df_diag, summary = diagnose_error_extremes_df(df_err)\n",
        "\n",
        "# Inspect problematic rows\n",
        "df_diag[df_diag[\"diagnostic_tag\"] != \"normal\"]\n",
        "\n",
        "# High-level view\n",
        "summary\n"
      ],
      "metadata": {
        "id": "xEe5YDOv0DMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify high error regions\n",
        "\n",
        "def diagnose_high_error_regions(df_err, bins=5):\n",
        "    \"\"\"\n",
        "    Identify high-error regions across target value ranges.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_err : pd.DataFrame\n",
        "        Must contain 'y_true', 'abs_error', 'pct_error'\n",
        "    bins : int\n",
        "        Number of quantile bins\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Columns:\n",
        "        - target_bin\n",
        "        - mean_abs_error\n",
        "        - mean_mape\n",
        "        - count\n",
        "        - error_contribution_pct\n",
        "    \"\"\"\n",
        "    total_abs_error = df_err[\"abs_error\"].sum()\n",
        "\n",
        "    target_bins = pd.qcut(\n",
        "        df_err[\"y_true\"],\n",
        "        bins,\n",
        "        duplicates=\"drop\"\n",
        "    )\n",
        "\n",
        "    target_error_regions_df = (\n",
        "        df_err\n",
        "        .assign(target_bin=target_bins)\n",
        "        .groupby(\"target_bin\", observed=True)\n",
        "        .agg(\n",
        "            mean_abs_error=(\"abs_error\", \"mean\"),\n",
        "            mean_mape=(\"pct_error\", \"mean\"),\n",
        "            count=(\"abs_error\", \"size\"),\n",
        "            total_abs_error_bin=(\"abs_error\", \"sum\")\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    target_error_regions_df[\"error_contribution_pct\"] = (\n",
        "        target_error_regions_df[\"total_abs_error_bin\"]\n",
        "        / (total_abs_error + 1e-9)\n",
        "    ) * 100\n",
        "\n",
        "    # optional: drop intermediate column\n",
        "    target_error_regions_df = target_error_regions_df.drop(\n",
        "        columns=[\"total_abs_error_bin\"]\n",
        "    )\n",
        "\n",
        "    return target_error_regions_df\n"
      ],
      "metadata": {
        "id": "2tJZeFU-1Rdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_error_df = diagnose_high_error_regions(df_err)\n",
        "\n",
        "high_error_df\n"
      ],
      "metadata": {
        "id": "_z1-LM_l1Yb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_target_space_error_diagnostics(\n",
        "    df_err,\n",
        "    target_bins=5,\n",
        "    z_thresh=2.5,\n",
        "    top_frac=0.01,\n",
        "    dominance_thresh=0.3\n",
        "):\n",
        "    \"\"\"\n",
        "    Target-space error diagnostics module.\n",
        "\n",
        "    Focuses ONLY on issues related to:\n",
        "    - target scale\n",
        "    - high-error regions\n",
        "    - outliers\n",
        "    - loss dominance\n",
        "    - optimization instability\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_err : pd.DataFrame\n",
        "        Must contain y_true, y_pred, error metrics\n",
        "    target_bins : int\n",
        "        Number of bins for target-space analysis\n",
        "    z_thresh : float\n",
        "        Z-score threshold for abs-error outliers\n",
        "    top_frac : float\n",
        "        Fraction of rows considered loss-dominant\n",
        "    dominance_thresh : float\n",
        "        Loss fraction threshold for instability\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict of DataFrames\n",
        "    \"\"\"\n",
        "\n",
        "    outputs = {}\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1. High-error regions in target space\n",
        "    # --------------------------------------------------\n",
        "    high_error_regions_df = diagnose_high_error_regions(\n",
        "        df_err,\n",
        "        bins=target_bins\n",
        "    )\n",
        "\n",
        "    outputs[\"target_high_error_regions\"] = high_error_regions_df\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2. Outliers, loss dominance, instability (row-level)\n",
        "    # --------------------------------------------------\n",
        "    df_extremes, extremes_summary_df = diagnose_error_extremes_df(\n",
        "        df_err,\n",
        "        z_thresh=z_thresh,\n",
        "        top_frac=top_frac,\n",
        "        dominance_thresh=dominance_thresh\n",
        "    )\n",
        "\n",
        "    outputs[\"target_extreme_rows\"] = df_extremes\n",
        "    outputs[\"target_extremes_summary\"] = extremes_summary_df\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 3. Compact executive summary (target-space only)\n",
        "    # --------------------------------------------------\n",
        "    summary_rows = []\n",
        "\n",
        "    # High-error region signal\n",
        "    worst_bin = high_error_regions_df.sort_values(\n",
        "        \"error_contribution_pct\", ascending=False\n",
        "    ).iloc[0]\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"issue_type\": \"high_error_target_region\",\n",
        "        \"detail\": str(worst_bin[\"target_bin\"]),\n",
        "        \"error_contribution_pct\": worst_bin[\"error_contribution_pct\"],\n",
        "        \"mean_abs_error\": worst_bin[\"mean_abs_error\"],\n",
        "        \"mean_mape\": worst_bin[\"mean_mape\"]\n",
        "    })\n",
        "\n",
        "    # Outlier signal\n",
        "    outlier_frac = extremes_summary_df.loc[\n",
        "        extremes_summary_df[\"diagnostic\"] == \"abs_error_outliers\",\n",
        "        \"fraction_of_rows\"\n",
        "    ].values[0]\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"issue_type\": \"rare_extreme_errors\",\n",
        "        \"detail\": \"abs_error_outliers\",\n",
        "        \"fraction_of_rows\": outlier_frac\n",
        "    })\n",
        "\n",
        "    # Loss dominance / instability\n",
        "    loss_row = extremes_summary_df.loc[\n",
        "        extremes_summary_df[\"diagnostic\"] == \"loss_dominance\"\n",
        "    ].iloc[0]\n",
        "\n",
        "    instab_row = extremes_summary_df.loc[\n",
        "        extremes_summary_df[\"diagnostic\"] == \"optimization_instability\"\n",
        "    ].iloc[0]\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"issue_type\": \"loss_dominance\",\n",
        "        \"detail\": f\"top_{int(top_frac*100)}pct_rows\",\n",
        "        \"loss_fraction\": loss_row[\"loss_fraction\"]\n",
        "    })\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"issue_type\": \"optimization_instability\",\n",
        "        \"detail\": \"global\",\n",
        "        \"flag\": instab_row[\"flag\"]\n",
        "    })\n",
        "\n",
        "    outputs[\"target_space_summary\"] = pd.DataFrame(summary_rows)\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "MD60TG8j1i9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: target-space diagnostics\n",
        "target_outputs = run_target_space_error_diagnostics(df_err)\n",
        "\n",
        "# Inspect\n",
        "target_outputs[\"target_high_error_regions\"]\n",
        "target_outputs[\"target_space_summary\"]\n",
        "\n",
        "# Drill down\n",
        "target_outputs[\"target_extreme_rows\"][\n",
        "    target_outputs[\"target_extreme_rows\"][\"diagnostic_tag\"] != \"normal\"\n",
        "].head()\n"
      ],
      "metadata": {
        "id": "jpy-e81U1lqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target_outputs[\"target_extreme_rows\"].to_csv('extreme_rows.csv', index=False)\n",
        "\n",
        "# submission_df_rf\n"
      ],
      "metadata": {
        "id": "vqqUgAbEpeRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature transformation diagnostics-numerical\n",
        "\n",
        "def diagnose_feature_transformations(X, df_err, features, bins=6):\n",
        "    rows = []\n",
        "\n",
        "    for feat in features:\n",
        "        df = pd.concat(\n",
        "            [X[[feat]], df_err[[\"abs_error\"]]],\n",
        "            axis=1\n",
        "        ).dropna()\n",
        "\n",
        "        df[\"bin\"] = pd.qcut(df[feat], bins, duplicates=\"drop\")\n",
        "        stats = df.groupby(\"bin\", observed=True)[\"abs_error\"].agg([\"mean\", \"std\"])\n",
        "\n",
        "        rows.append({\n",
        "            \"feature\": feat,\n",
        "            \"error_range\": stats[\"mean\"].max() - stats[\"mean\"].min(),\n",
        "            \"error_std_ratio\": stats[\"std\"].max() / (stats[\"std\"].min() + 1e-6)\n",
        "        })\n",
        "\n",
        "    return (\n",
        "        pd.DataFrame(rows)\n",
        "        .sort_values(\"error_range\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "\n",
        "# feature inertaction diagnostics\n",
        "\n",
        "def diagnose_feature_interactions(shap_interaction_error_df, top_k=10):\n",
        "    return (\n",
        "        shap_interaction_error_df\n",
        "        .head(top_k)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "\n",
        "# feature removal  diagnostics\n",
        "def diagnose_feature_removal(\n",
        "    shap_df,\n",
        "    shap_error_corr_df,\n",
        "    pdp_summary,\n",
        "    shap_thresh=0.01,\n",
        "    error_corr_thresh=0.05\n",
        "):\n",
        "    rows = []\n",
        "\n",
        "    for feat in shap_df.columns:\n",
        "        mean_shap = shap_df[feat].abs().mean()\n",
        "\n",
        "        err_corr = shap_error_corr_df.loc[\n",
        "            shap_error_corr_df[\"feature\"] == feat,\n",
        "            \"shap_abs_error_corr\"\n",
        "        ].values[0]\n",
        "\n",
        "        pdp_shape = pdp_summary.get(feat, {}).get(\"shape\", \"unknown\")\n",
        "\n",
        "        rows.append({\n",
        "            \"feature\": feat,\n",
        "            \"mean_abs_shap\": mean_shap,\n",
        "            \"shap_error_corr\": err_corr,\n",
        "            \"pdp_shape\": pdp_shape,\n",
        "            \"removal_candidate\": (\n",
        "                mean_shap < shap_thresh\n",
        "                and abs(err_corr) < error_corr_thresh\n",
        "            )\n",
        "        })\n",
        "\n",
        "    return (\n",
        "        pd.DataFrame(rows)\n",
        "        .sort_values(\"mean_abs_shap\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "\n",
        "# feature subpopulation diagnostics\n",
        "\n",
        "def diagnose_feature_subpopulations(X, df_err, feature, bins=5):\n",
        "    df = pd.concat(\n",
        "        [X[[feature]], df_err[[\"abs_error\"]]],\n",
        "        axis=1\n",
        "    ).dropna()\n",
        "\n",
        "    df[\"bin\"] = pd.qcut(df[feature], bins, duplicates=\"drop\")\n",
        "\n",
        "    return (\n",
        "        df.groupby(\"bin\", observed=True)\n",
        "        .agg(\n",
        "            mean_abs_error=(\"abs_error\", \"mean\"),\n",
        "            count=(\"abs_error\", \"size\")\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "# extrapolation diagnostics\n",
        "\n",
        "# distance calculation\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def compute_data_coverage_distance(X_train, X_test, k=5):\n",
        "    nn = NearestNeighbors(n_neighbors=k)\n",
        "    nn.fit(X_train)\n",
        "\n",
        "    distances, _ = nn.kneighbors(X_test)\n",
        "    return distances.mean(axis=1)\n",
        "\n",
        "\n",
        "# extrapolation vs error\n",
        "\n",
        "def diagnose_extrapolation(df_err, coverage_distances, quantile=0.95):\n",
        "    df = df_err.copy()\n",
        "    df[\"coverage_distance\"] = coverage_distances\n",
        "\n",
        "    threshold = np.quantile(coverage_distances, quantile)\n",
        "    df[\"is_extrapolation\"] = df[\"coverage_distance\"] > threshold\n",
        "\n",
        "    summary = (\n",
        "        df.groupby(\"is_extrapolation\")\n",
        "        .agg(\n",
        "            mean_abs_error=(\"abs_error\", \"mean\"),\n",
        "            count=(\"abs_error\", \"size\")\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    return df, summary\n"
      ],
      "metadata": {
        "id": "FnskEgFZ6Hsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_feature_space_diagnostics(\n",
        "    X_train,\n",
        "    X_test,\n",
        "    df_err,\n",
        "    shap_df,\n",
        "    shap_error_corr_df,\n",
        "    shap_interaction_error_df,\n",
        "    pdp_summary,\n",
        "    top_features,\n",
        "    bins=6\n",
        "):\n",
        "    outputs = {}\n",
        "\n",
        "    # 1. Transformations\n",
        "    outputs[\"feature_transformation_signals\"] = (\n",
        "        diagnose_feature_transformations(\n",
        "            X_test, df_err, top_features, bins\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 2. Interactions\n",
        "    outputs[\"feature_interaction_signals\"] = (\n",
        "        diagnose_feature_interactions(\n",
        "            shap_interaction_error_df\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 3. Removal candidates\n",
        "    outputs[\"feature_removal_signals\"] = (\n",
        "        diagnose_feature_removal(\n",
        "            shap_df,\n",
        "            shap_error_corr_df,\n",
        "            pdp_summary\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 4. Subpopulations\n",
        "    outputs[\"feature_subpopulation_signals\"] = {\n",
        "        feat: diagnose_feature_subpopulations(\n",
        "            X_test, df_err, feat\n",
        "        )\n",
        "        for feat in top_features\n",
        "    }\n",
        "\n",
        "    # 5. Data coverage / extrapolation\n",
        "    coverage_dist = compute_data_coverage_distance(\n",
        "        X_train, X_test\n",
        "    )\n",
        "\n",
        "    extrap_df, extrap_summary = diagnose_extrapolation(\n",
        "        df_err, coverage_dist\n",
        "    )\n",
        "\n",
        "    outputs[\"data_coverage_rows\"] = extrap_df\n",
        "    outputs[\"data_coverage_summary\"] = extrap_summary\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "S0_YFy806NPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_features = dfs['shap_error_correlations'].head(20)['feature'].tolist()\n",
        "\n",
        "feature_outputs = run_feature_space_diagnostics(\n",
        "    X_train=X_train,\n",
        "    X_test=X_test,\n",
        "    df_err=df_err,\n",
        "    shap_df=dfs['shap_values'],\n",
        "    shap_error_corr_df=dfs['shap_error_correlations'],\n",
        "    shap_interaction_error_df=dfs['shap_interaction_error'],\n",
        "    pdp_summary=results['pdp_summary'],\n",
        "    top_features=top_features,\n",
        "    bins=6\n",
        ")\n"
      ],
      "metadata": {
        "id": "pdMGZGou6SCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_outputs.keys()"
      ],
      "metadata": {
        "id": "kNmTRRat6UxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_outputs[\"feature_subpopulation_signals\"][top_features[0]]"
      ],
      "metadata": {
        "id": "mGKdacjZ6cyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_outputs['feature_transformation_signals']"
      ],
      "metadata": {
        "id": "RMC3UQ7J6hci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}